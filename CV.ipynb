{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "for a in ['', '/Users/kristjan.roosild/opt/miniconda3/envs/donkey/lib/python37.zip',\n",
    "    '/Users/kristjan.roosild/opt/miniconda3/envs/donkey/lib/python3.7',\n",
    "    '/Users/kristjan.roosild/opt/miniconda3/envs/donkey/lib/python3.7/lib-dynload',\n",
    "    '/Users/kristjan.roosild/.local/lib/python3.7/site-packages',\n",
    "    '/Users/kristjan.roosild/opt/miniconda3/envs/donkey/lib/python3.7/site-packages',\n",
    "    '/Users/kristjan.roosild/projects/donkeycar',\n",
    "    '/Users/kristjan.roosild/opt/miniconda3/envs/donkey/lib/python3.7/site-packages/locket-0.2.1-py3.7.egg']:\n",
    "    sys.path.append(a)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "from donkeycar.parts.keras import KerasCategorical, KerasLinear, KerasLSTM, Keras3D_CNN, KerasLinearOnlySteering, KerasInterpreter\n",
    "\n",
    "import donkeycar as dk\n",
    "\n",
    "import math\n",
    "import os\n",
    "from typing import List, Dict, Union, Tuple\n",
    "\n",
    "from donkeycar.config import Config\n",
    "from donkeycar.parts.keras import KerasPilot\n",
    "from donkeycar.pipeline.database import PilotDatabase\n",
    "from donkeycar.pipeline.sequence import TubRecord, TubSequence, TfmIterator\n",
    "from donkeycar.pipeline.types import TubDataset\n",
    "from donkeycar.pipeline.augmentations import ImageAugmentation\n",
    "from donkeycar.utils import normalize_image, train_test_split\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class BatchSequence(object):\n",
    "    \"\"\"\n",
    "    The idea is to have a shallow sequence with types that can hydrate\n",
    "    themselves to np.ndarray initially and later into the types required by\n",
    "    tf.data (i.e. dictionaries or np.ndarrays).\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 model: KerasPilot,\n",
    "                 config: Config,\n",
    "                 records: List[TubRecord],\n",
    "                 is_train: bool) -> None:\n",
    "        self.model = model\n",
    "        self.config = config\n",
    "        self.sequence = TubSequence(records)\n",
    "        self.batch_size = self.config.BATCH_SIZE\n",
    "        self.is_train = is_train\n",
    "        self.augmentation = ImageAugmentation(config, 'AUGMENTATIONS')\n",
    "        self.transformation = ImageAugmentation(config, 'TRANSFORMATIONS')\n",
    "        self.pipeline = self._create_pipeline()\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return math.ceil(len(self.pipeline) / self.batch_size)\n",
    "\n",
    "    def image_processor(self, img_arr):\n",
    "        \"\"\" Transformes the images and augments if in training. Then\n",
    "            normalizes it. \"\"\"\n",
    "        img_arr = self.transformation.run(img_arr)\n",
    "        if self.is_train:\n",
    "            img_arr = self.augmentation.run(img_arr)\n",
    "        norm_img = normalize_image(img_arr)\n",
    "        return norm_img\n",
    "\n",
    "    def _create_pipeline(self) -> TfmIterator:\n",
    "        \"\"\" This can be overridden if more complicated pipelines are\n",
    "            required \"\"\"\n",
    "        # 1. Initialise TubRecord -> x, y transformations\n",
    "        def get_x(record: TubRecord) -> Dict[str, Union[float, np.ndarray]]:\n",
    "            \"\"\" Extracting x from record for training\"\"\"\n",
    "            out_tuple = self.model.x_transform_and_process(\n",
    "                record, self.image_processor)\n",
    "            # convert tuple to dictionary which is understood by tf.data\n",
    "            out_dict = self.model.x_translate(out_tuple)\n",
    "            return out_dict\n",
    "\n",
    "        def get_y(record: TubRecord) -> Dict[str, Union[float, np.ndarray]]:\n",
    "            \"\"\" Extracting y from record for training \"\"\"\n",
    "            y0 = self.model.y_transform(record)\n",
    "            y1 = self.model.y_translate(y0)\n",
    "            return y1\n",
    "\n",
    "        # 2. Build pipeline using the transformations\n",
    "        pipeline = self.sequence.build_pipeline(x_transform=get_x,\n",
    "                                                y_transform=get_y)\n",
    "        return pipeline\n",
    "\n",
    "    def create_tf_data(self) -> tf.data.Dataset:\n",
    "        \"\"\" Assembles the tf data pipeline \"\"\"\n",
    "        dataset = tf.data.Dataset.from_generator(\n",
    "            generator=lambda: self.pipeline,\n",
    "            output_types=self.model.output_types(),\n",
    "            output_shapes=self.model.output_shapes())\n",
    "        return dataset.repeat().batch(self.batch_size)\n",
    "\n",
    "\n",
    "def get_model_train_details(database: PilotDatabase, model: str = None) -> Tuple[str, int]:\n",
    "    if not model:\n",
    "        model_name, model_num = database.generate_model_name()\n",
    "    else:\n",
    "        model_name, model_num = os.path.abspath(model), 0\n",
    "    return model_name, model_num\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading config file: /Users/kristjan.roosild/mycar/config.py\n",
      "loading personal config over-rides from myconfig.py\n",
      "Using catalog /Users/kristjan.roosild/OneDrive/kool/maka/data/2-2-CW-90/catalog_2.catalog\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cfg = dk.load_config(config_path='/Users/kristjan.roosild/mycar/config.py')\n",
    "\n",
    "cfg.TRANSFORMATIONS = ['CROP']\n",
    "cfg.ROI_CROP_TOP = 60\n",
    "\n",
    "input_shape = (cfg.IMAGE_H, cfg.IMAGE_W, cfg.IMAGE_DEPTH)\n",
    "\n",
    "tubs_names = [\n",
    "    # '1-1-CC-80',\n",
    "    # '1-2-CC-90',\n",
    "    # '1-3-CC-85',\n",
    "    # '2-1-CW-80',\n",
    "    '2-2-CW-90',\n",
    "    # '2-3-CW-85',\n",
    "    # '3-2-CW-90',\n",
    "    # '3-3-CW-85',\n",
    "    # '4-1-CC-80',\n",
    "    # '4-2-CC-90',\n",
    "    # '4-3-CC-85'\n",
    "]\n",
    "\n",
    "tubs = []\n",
    "\n",
    "for tub_name in tubs_names:\n",
    "    tubs.append('/Users/kristjan.roosild/OneDrive/kool/maka/data/' + tub_name)\n",
    "\n",
    "all_tub_paths = [os.path.expanduser(tub) for tub in tubs]\n",
    "dataset = TubDataset(config=cfg, tub_paths=all_tub_paths, seq_size=0)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [],
   "source": [
    "\n",
    "def prep_fold_data(kl, cfg, data):\n",
    "    training_records, validation_records = train_test_split(data, shuffle=True,\n",
    "                                                            test_size=(1. - cfg.TRAIN_TEST_SPLIT))\n",
    "    print(f'Records # Training {len(training_records)}')\n",
    "    print(f'Records # Validation {len(validation_records)}')\n",
    "    # We need augmentation in validation when using crop / trapeze\n",
    "    training_pipe = BatchSequence(kl, cfg, training_records, is_train=True)\n",
    "    validation_pipe = BatchSequence(kl, cfg, validation_records, is_train=False)\n",
    "    tune = tf.data.experimental.AUTOTUNE\n",
    "    dataset_train = training_pipe.create_tf_data().prefetch(tune)\n",
    "    dataset_validate = validation_pipe.create_tf_data().prefetch(tune)\n",
    "    train_size = len(training_pipe)\n",
    "    val_size = len(validation_pipe)\n",
    "    assert val_size > 0, \"Not enough validation data, decrease the batch size or add more data.\"\n",
    "    return dataset_train, dataset_validate, train_size, val_size\n",
    "\n",
    "import wandb\n",
    "\n",
    "def init_wandb():\n",
    "    config = {\n",
    "        \"fold\": fold,\n",
    "        \"model\": str(kl),\n",
    "        \"tubs\": ','.join(tubs),\n",
    "    }\n",
    "    wandb.init(project=\"master-thesis\", entity=\"kristjan\", config=config)\n",
    "\n",
    "\n",
    "def train(kl, cfg, data):\n",
    "    dataset_train, dataset_validate, train_size, val_size = prep_fold_data(kl, cfg, data)\n",
    "    init_wandb()\n",
    "    history = kl.train(model_path=model_path,\n",
    "                       train_data=dataset_train,\n",
    "                       train_steps=train_size,\n",
    "                       batch_size=cfg.BATCH_SIZE,\n",
    "                       validation_data=dataset_validate,\n",
    "                       validation_steps=val_size,\n",
    "                       epochs=cfg.MAX_EPOCHS,\n",
    "                       verbose=cfg.VERBOSE_TRAIN,\n",
    "                       min_delta=cfg.MIN_DELTA,\n",
    "                       patience=cfg.EARLY_STOP_PATIENCE,\n",
    "                       show_plot=cfg.SHOW_PLOT)\n",
    "\n",
    "    return history\n",
    "\n",
    "def mse(v1, v2):\n",
    "    return np.mean((np.array(v1) - np.array(v2)) ** 2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:donkeycar.pipeline.augmentations:Creating augmentation CROP with ROI_CROP L: 0, R: 0, B: 0, T: 60\n",
      "INFO:donkeycar.parts.keras:Created KerasLinearOnlySteering with interpreter: KerasInterpreter\n",
      "INFO:donkeycar.pipeline.augmentations:Creating augmentation CROP with ROI_CROP L: 0, R: 0, B: 0, T: 60\n",
      "INFO:donkeycar.pipeline.augmentations:Creating augmentation CROP with ROI_CROP L: 0, R: 0, B: 0, T: 60\n",
      "INFO:donkeycar.pipeline.augmentations:Creating augmentation CROP with ROI_CROP L: 0, R: 0, B: 0, T: 60\n",
      "INFO:donkeycar.pipeline.augmentations:Creating augmentation CROP with ROI_CROP L: 0, R: 0, B: 0, T: 60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Records # Training 2997\n",
      "Records # Validation 750\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Finishing last run (ID:1wntih9y) before initializing another..."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<br/>Waiting for W&B process to finish, PID 29375... <strong style=\"color:green\">(success).</strong>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "VBox(children=(Label(value=' 9.44MB of 9.44MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c591d5bbdf1d480694479bbfd22c9ff0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\">\n<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▂▃▃▄▅▅▆▆▇▇█</td></tr><tr><td>loss</td><td>█▆▆▅▅▅▄▄▃▃▁▁▁</td></tr><tr><td>n_outputs0_loss</td><td>█▆▆▅▅▅▄▄▃▃▁▁▁</td></tr><tr><td>n_outputs1_loss</td><td>███▇▆▄▃▂▂▂▁▂▂</td></tr><tr><td>val_loss</td><td>▃▃▂▃▂▂▅▁▂▂█▃▃</td></tr><tr><td>val_n_outputs0_loss</td><td>▃▃▂▃▂▂▅▁▂▂█▃▃</td></tr><tr><td>val_n_outputs1_loss</td><td>▇▆▇█▆▅▁▂▂▄▆▆▆</td></tr></table><br/></div><div class=\"wandb-col\">\n<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>7</td></tr><tr><td>best_val_loss</td><td>0.24788</td></tr><tr><td>epoch</td><td>12</td></tr><tr><td>loss</td><td>0.21081</td></tr><tr><td>n_outputs0_loss</td><td>0.21081</td></tr><tr><td>n_outputs1_loss</td><td>0.87386</td></tr><tr><td>val_loss</td><td>0.26486</td></tr><tr><td>val_n_outputs0_loss</td><td>0.26486</td></tr><tr><td>val_n_outputs1_loss</td><td>0.92907</td></tr></table>\n</div></div>\nSynced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n<br/>Synced <strong style=\"color:#cdcd00\">solar-gorge-4</strong>: <a href=\"https://wandb.ai/kristjan/master-thesis/runs/1wntih9y\" target=\"_blank\">https://wandb.ai/kristjan/master-thesis/runs/1wntih9y</a><br/>\nFind logs at: <code>./wandb/run-20220212_011608-1wntih9y/logs</code><br/>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Successfully finished last run (ID:1wntih9y). Initializing new run:<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "transformation = ImageAugmentation(cfg, 'TRANSFORMATIONS')\n",
    "\n",
    "kf = KFold(n_splits=2)\n",
    "\n",
    "records = dataset.get_records()\n",
    "\n",
    "for fold, (train_index, test_index) in enumerate(kf.split(records)):\n",
    "    model_name = f'test-fold{fold}'\n",
    "    model_path = f'/Users/kristjan.roosild/OneDrive/kool/maka/models/{model_name}.h5'\n",
    "    train_data = [records[i] for i in train_index]\n",
    "    kl = KerasLinearOnlySteering(interpreter=KerasInterpreter(), input_shape=input_shape)\n",
    "    train(kl, cfg, train_data)\n",
    "\n",
    "    test_records = [records[i] for i in test_index]\n",
    "    test_preds = []\n",
    "    for r in test_records:\n",
    "        test_image = transformation.run(r.image())\n",
    "        test_image = normalize_image(test_image)\n",
    "        test_pred = kl.inference(test_image, None)[0]\n",
    "        test_preds.append(test_pred)\n",
    "\n",
    "    ground_truth = [r.underlying['user/angle'] for r in test_records]\n",
    "    fold_mse = mse(ground_truth, test_preds)\n",
    "    wandb.run.summary[\"test_loss\"] = fold_mse\n",
    "    print(f'mse for fold {fold} is {fold_mse}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}