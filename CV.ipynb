{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "for a in ['', '/Users/kristjan.roosild/opt/miniconda3/envs/donkey/lib/python37.zip',\n",
    "    '/Users/kristjan.roosild/opt/miniconda3/envs/donkey/lib/python3.7',\n",
    "    '/Users/kristjan.roosild/opt/miniconda3/envs/donkey/lib/python3.7/lib-dynload',\n",
    "    '/Users/kristjan.roosild/.local/lib/python3.7/site-packages',\n",
    "    '/Users/kristjan.roosild/opt/miniconda3/envs/donkey/lib/python3.7/site-packages',\n",
    "    '/Users/kristjan.roosild/projects/donkeycar',\n",
    "    '/Users/kristjan.roosild/opt/miniconda3/envs/donkey/lib/python3.7/site-packages/locket-0.2.1-py3.7.egg']:\n",
    "    sys.path.append(a)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from donkeycar.parts.keras import KerasCategorical, KerasLinear, KerasLSTM, Keras3D_CNN, KerasLinearOnlySteering, KerasInterpreter\n",
    "\n",
    "import donkeycar as dk\n",
    "\n",
    "import math\n",
    "import os\n",
    "from typing import List, Dict, Union, Tuple\n",
    "\n",
    "from donkeycar.config import Config\n",
    "from donkeycar.parts.keras import KerasPilot\n",
    "from donkeycar.pipeline.database import PilotDatabase\n",
    "from donkeycar.pipeline.sequence import TubRecord, TubSequence, TfmIterator\n",
    "from donkeycar.pipeline.types import TubDataset\n",
    "from donkeycar.pipeline.augmentations import ImageAugmentation\n",
    "from donkeycar.utils import normalize_image, train_test_split\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class BatchSequence(object):\n",
    "    \"\"\"\n",
    "    The idea is to have a shallow sequence with types that can hydrate\n",
    "    themselves to np.ndarray initially and later into the types required by\n",
    "    tf.data (i.e. dictionaries or np.ndarrays).\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 model: KerasPilot,\n",
    "                 config: Config,\n",
    "                 records: List[TubRecord],\n",
    "                 is_train: bool) -> None:\n",
    "        self.model = model\n",
    "        self.config = config\n",
    "        self.sequence = TubSequence(records)\n",
    "        self.batch_size = self.config.BATCH_SIZE\n",
    "        self.is_train = is_train\n",
    "        self.augmentation = ImageAugmentation(config, 'AUGMENTATIONS')\n",
    "        self.transformation = ImageAugmentation(config, 'TRANSFORMATIONS')\n",
    "        self.pipeline = self._create_pipeline()\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return math.ceil(len(self.pipeline) / self.batch_size)\n",
    "\n",
    "    def image_processor(self, img_arr):\n",
    "        \"\"\" Transformes the images and augments if in training. Then\n",
    "            normalizes it. \"\"\"\n",
    "        img_arr = self.transformation.run(img_arr)\n",
    "        if self.is_train:\n",
    "            img_arr = self.augmentation.run(img_arr)\n",
    "        norm_img = normalize_image(img_arr)\n",
    "        return norm_img\n",
    "\n",
    "    def _create_pipeline(self) -> TfmIterator:\n",
    "        \"\"\" This can be overridden if more complicated pipelines are\n",
    "            required \"\"\"\n",
    "        # 1. Initialise TubRecord -> x, y transformations\n",
    "        def get_x(record: TubRecord) -> Dict[str, Union[float, np.ndarray]]:\n",
    "            \"\"\" Extracting x from record for training\"\"\"\n",
    "            out_tuple = self.model.x_transform_and_process(\n",
    "                record, self.image_processor)\n",
    "            # convert tuple to dictionary which is understood by tf.data\n",
    "            out_dict = self.model.x_translate(out_tuple)\n",
    "            return out_dict\n",
    "\n",
    "        def get_y(record: TubRecord) -> Dict[str, Union[float, np.ndarray]]:\n",
    "            \"\"\" Extracting y from record for training \"\"\"\n",
    "            y0 = self.model.y_transform(record)\n",
    "            y1 = self.model.y_translate(y0)\n",
    "            return y1\n",
    "\n",
    "        # 2. Build pipeline using the transformations\n",
    "        pipeline = self.sequence.build_pipeline(x_transform=get_x,\n",
    "                                                y_transform=get_y)\n",
    "        return pipeline\n",
    "\n",
    "    def create_tf_data(self) -> tf.data.Dataset:\n",
    "        \"\"\" Assembles the tf data pipeline \"\"\"\n",
    "        dataset = tf.data.Dataset.from_generator(\n",
    "            generator=lambda: self.pipeline,\n",
    "            output_types=self.model.output_types(),\n",
    "            output_shapes=self.model.output_shapes())\n",
    "        return dataset.repeat().batch(self.batch_size)\n",
    "\n",
    "\n",
    "def get_model_train_details(database: PilotDatabase, model: str = None) -> Tuple[str, int]:\n",
    "    if not model:\n",
    "        model_name, model_num = database.generate_model_name()\n",
    "    else:\n",
    "        model_name, model_num = os.path.abspath(model), 0\n",
    "    return model_name, model_num\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "cfg = dk.load_config(config_path='/Users/kristjan.roosild/mycar/config.py')\n",
    "\n",
    "cfg.TRANSFORMATIONS = ['CROP']\n",
    "cfg.ROI_CROP_TOP = 60\n",
    "\n",
    "input_shape = (cfg.IMAGE_H, cfg.IMAGE_W, cfg.IMAGE_DEPTH)\n",
    "\n",
    "tubs_names_80_speed = [\n",
    "    '1-1-CC-80',\n",
    "    '2-1-CW-80',\n",
    "    '4-1-CC-80',\n",
    "]\n",
    "\n",
    "tubs_names_85_speed = [\n",
    "    '1-3-CC-85',\n",
    "    '2-3-CW-85',\n",
    "    '3-3-CW-85',\n",
    "    '4-3-CC-85'\n",
    "]\n",
    "\n",
    "tub_names_90_speed = [\n",
    "    '1-2-CC-90',\n",
    "    '2-2-CW-90',\n",
    "    '3-2-CW-90',\n",
    "    '4-2-CC-90',\n",
    "]\n",
    "\n",
    "\n",
    "def load_records(tub_name):\n",
    "    return TubDataset(\n",
    "        config=cfg,\n",
    "        tub_paths=[os.path.expanduser('/Users/kristjan.roosild/OneDrive/kool/maka/data/' + tub_name)],\n",
    "        seq_size=0).get_records()\n",
    "\n",
    "\n",
    "tub_records = {tn: load_records(tn) for tn in tubs_names_80_speed + tubs_names_85_speed + tub_names_90_speed}\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "def prep_fold_data(kl, cfg, data):\n",
    "    training_records, validation_records = train_test_split(data, shuffle=True,\n",
    "                                                            test_size=(1. - cfg.TRAIN_TEST_SPLIT))\n",
    "    print(f'Records # Training {len(training_records)}')\n",
    "    print(f'Records # Validation {len(validation_records)}')\n",
    "    # We need augmentation in validation when using crop / trapeze\n",
    "    training_pipe = BatchSequence(kl, cfg, training_records, is_train=True)\n",
    "    validation_pipe = BatchSequence(kl, cfg, validation_records, is_train=False)\n",
    "    tune = tf.data.experimental.AUTOTUNE\n",
    "    dataset_train = training_pipe.create_tf_data().prefetch(tune)\n",
    "    dataset_validate = validation_pipe.create_tf_data().prefetch(tune)\n",
    "    train_size = len(training_pipe)\n",
    "    val_size = len(validation_pipe)\n",
    "    assert val_size > 0, \"Not enough validation data, decrease the batch size or add more data.\"\n",
    "    return dataset_train, dataset_validate, train_size, val_size\n",
    "\n",
    "import wandb\n",
    "\n",
    "def init_wandb():\n",
    "    config = {\n",
    "        \"fold\": fold,\n",
    "        \"model\": str(kl),\n",
    "        \"tubs\": ','.join(tubs),\n",
    "    }\n",
    "    wandb.init(project=\"master-thesis\", entity=\"kristjan\", config=config)\n",
    "\n",
    "\n",
    "def train(kl, cfg, data):\n",
    "    dataset_train, dataset_validate, train_size, val_size = prep_fold_data(kl, cfg, data)\n",
    "    init_wandb()\n",
    "    history = kl.train(model_path=model_path,\n",
    "                       train_data=dataset_train,\n",
    "                       train_steps=train_size,\n",
    "                       batch_size=cfg.BATCH_SIZE,\n",
    "                       validation_data=dataset_validate,\n",
    "                       validation_steps=val_size,\n",
    "                       epochs=cfg.MAX_EPOCHS,\n",
    "                       verbose=cfg.VERBOSE_TRAIN,\n",
    "                       min_delta=cfg.MIN_DELTA,\n",
    "                       patience=cfg.EARLY_STOP_PATIENCE,\n",
    "                       show_plot=cfg.SHOW_PLOT)\n",
    "\n",
    "    return history\n",
    "\n",
    "def mse(v1, v2):\n",
    "    return np.mean((np.array(v1) - np.array(v2)) ** 2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "N_FOLDS = 10\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits=N_FOLDS)\n",
    "\n",
    "training_folds = defaultdict(list)\n",
    "testing_folds = defaultdict(list)\n",
    "\n",
    "for tub_name in tub_names_90_speed:\n",
    "    records = tub_records[tub_name]\n",
    "    for fold, (train_index, test_index) in enumerate(kf.split(records)):\n",
    "        train_records = [records[i] for i in train_index]\n",
    "        training_folds[fold].extend(train_records)\n",
    "        test_records = [records[i] for i in test_index]\n",
    "        testing_folds[fold].extend(test_records)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "all_records_85_speed = []\n",
    "for n in tubs_names_85_speed:\n",
    "    all_records_85_speed.extend(tub_records[n])\n",
    "\n",
    "all_records_80_speed = []\n",
    "for n in tubs_names_80_speed:\n",
    "    all_records_80_speed.extend(tub_records[n])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.enable()\n",
    "\n",
    "def get_loss(kl, test_records):\n",
    "    test_preds = []\n",
    "    for r in test_records:\n",
    "        gc.collect()\n",
    "        test_image = transformation.run(r.image())\n",
    "        test_image = normalize_image(test_image)\n",
    "        test_pred = kl.inference(test_image, None)[0]\n",
    "        test_preds.append(test_pred)\n",
    "    ground_truth = [r.underlying['user/angle'] for r in test_records]\n",
    "    return mse(ground_truth, test_preds)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "transformation = ImageAugmentation(cfg, 'TRANSFORMATIONS')\n",
    "\n",
    "test_losses_for_80_speed = []\n",
    "test_losses_for_85_speed = []\n",
    "test_losses_for_90_speed = []\n",
    "\n",
    "for fold, train_records in training_folds.items():\n",
    "    model_name = f'test-fold{fold}'\n",
    "    model_path = f'/Users/kristjan.roosild/OneDrive/kool/maka/models/{model_name}.h5'\n",
    "    kl = KerasLinearOnlySteering(interpreter=KerasInterpreter(), input_shape=input_shape)\n",
    "    train(kl, cfg, train_records)\n",
    "\n",
    "    print(f'Getting 80-speed mse for fold {fold}')\n",
    "    fold_mse_80_speed = get_loss(kl, all_records_80_speed)\n",
    "    test_losses_for_80_speed.append(fold_mse_80_speed)\n",
    "    wandb.run.summary[\"80_speed_test_loss\"] = fold_mse_80_speed\n",
    "    print(f'80 speed mse for fold {fold} is {fold_mse_80_speed}')\n",
    "\n",
    "    print(f'Getting 85-speed mse for fold {fold}')\n",
    "    fold_mse_85_speed = get_loss(kl, all_records_85_speed)\n",
    "    test_losses_for_85_speed.append(fold_mse_85_speed)\n",
    "    wandb.run.summary[\"85_speed_test_loss\"] = fold_mse_85_speed\n",
    "    print(f'85 speed mse for fold {fold} is {fold_mse_85_speed}')\n",
    "\n",
    "    print(f'Getting 90-speed mse for fold {fold}')\n",
    "    fold_mse_90_speed = get_loss(kl, testing_folds[fold])\n",
    "    test_losses_for_90_speed.append(fold_mse_90_speed)\n",
    "    wandb.run.summary[\"90_speed_test_loss\"] = fold_mse_90_speed\n",
    "    print(f'90 speed mse for fold {fold} is {fold_mse_90_speed}')\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}